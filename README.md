Team Visionx ü§ü ‚Äì Smart Navigation & Object Detection AssistantEmpowering the Visually Impaired through AI-Driven Spatial Awareness.üìñ Table of ContentsOverviewKey FeaturesHow It WorksInstallationDeployment (Streamlit Cloud)Tech StackFuture RoadmapLicenseüîç OverviewTeam Visionx is an assistive technology web application designed to help visually impaired individuals navigate their surroundings. Using a custom-trained YOLO model, the app identifies hazards, moving objects, and clear paths, providing real-time Smart Navigation instructions via voice feedback.The goal is simple: Convert visual data into actionable audio guidance.‚ú® Key FeaturesSmart Navigation Brain: Beyond just naming objects, the app calculates object positions (Left, Center, Right) and provides instructions (e.g., "Car on the left. Move right.").Priority Detection: Danger classes (knives, fire, guns) are prioritized in the audio queue over standard objects.Triple Input Support: Works with live Webcams, uploaded images, and video files.Voice Feedback System: Built-in speech synthesis with a smart cooldown to prevent "audio clutter."Dynamic Dashboard: Real-time confidence sliders and toggleable settings for a personalized experience.üß† How It WorksThe app divides the camera frame into three vertical sectors:Left: $0\%$ to $33\%$ of frame width.Center: $33\%$ to $66\%$ of frame width (Triggering "Stop" commands).Right: $66\%$ to $100\%$ of frame width.If a "Danger" or "Moving" object enters the Center zone, the app immediately issues a "Stop" or "Move" instruction based on the clearest path available.üõ† Installation1. Clone the repositoryBashgit clone https://github.com/yourusername/visionx-app.git
cd visionx-app
2. Create a virtual environmentBashpython -m venv venv
source venv/bin/activate  # Linux/macOS
# OR
venv\Scripts\activate     # Windows
3. Install dependenciesBashpip install -r requirements.txt
4. Add your ModelPlace your trained best.pt file in the root directory.‚òÅÔ∏è Deployment (Streamlit Cloud)To run Visionx on the web, ensure your repository contains these files:app.py: The main Python script.requirements.txt: Include ultralytics, opencv-python-headless, and numpy.packages.txt: (Important for Linux audio) Include espeak and freeglut3-dev.best.pt: Your YOLO weights.üíª Tech StackCore: Python 3.9+AI Model: YOLO (Ultralytics)Interface: StreamlitComputer Vision: OpenCVSpeech: eSpeak / Subprocess / Web Speech APIüöÄ Future RoadmapHaptic Feedback: Integration with mobile vibration API for non-audio alerts.Depth Estimation: Implementing MiDaS to calculate exactly how many meters away an object is.GPS Integration: Real-time map navigation paired with local object detection.‚öñÔ∏è LicenseThis project is open-source. We believe assistive technology should be accessible to everyone. Feel free to fork, modify, and improve!Team Visionx üíô ‚Äì Seeing the world differently.
